{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d6bf6e9",
   "metadata": {},
   "source": [
    "20.5 9 Scale Your ML (DL) Prototype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77a16fd",
   "metadata": {},
   "source": [
    "In this step, your goal is to ensure that your ML/DL approach, which you’ve proved to be\n",
    "viable, can work with large volumes of data. You need to scale your prototype. Please\n",
    "work with your mentor to determine what that means for your problem.\n",
    "Using scikit-learn, SparkML, Keras, TensorFlow, PyTorch, or some of the other\n",
    "technologies you have learned, implement your prototype at scale.\n",
    "In case your earlier prototype was working with a subset, ensure that this scaled-up\n",
    "prototype can handle your complete dataset.\n",
    "Think about what your capstone problem would look like in the real world:\n",
    "● How much data would you need to handle?\n",
    "● Can you scale your prototype to handle that volume of data using the approach\n",
    "and tools you have selected?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e65f200",
   "metadata": {},
   "source": [
    "The submission shows that the student understands how to scale a machine learning or a deep learning model.\n",
    "The scaled prototype can handle the complete dataset that the student has collected (even if the student has only used a sample, so far) and is capable of handling all of the data that a real-world version of the application would need to handle.\n",
    "The submission demonstrates that the student made well-thought-out decisions about scaling their prototype:\n",
    "Choice of tools/libraries: scikit-learn, SparkML, TensorFlow, Keras, and PyTorch, etc.\n",
    "Choice of machine learning/deep learning technique\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c602357",
   "metadata": {},
   "source": [
    "Well-documented GitHub repository and code. The Jupyter notebooks for the code provide step-by-step documentation that’s easy to follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "d0d5a032",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383f7a2d",
   "metadata": {},
   "source": [
    "## Scaling \n",
    "* Vertical: Bigger CPU, mem, disk on one host\n",
    "* Horizontal: Processing is distributed to multiple hosts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ea506e",
   "metadata": {},
   "source": [
    "### Initalize Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "566fa298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "23ee360d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "5ed95aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SparkSession \n",
    "spark = SparkSession.builder \\\n",
    "      .master(\"local[1]\") \\\n",
    "      .appName(\"bizwiz\") \\\n",
    "      .getOrCreate() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40a5adc",
   "metadata": {},
   "source": [
    "### Open \"Big\" dataset with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "3c0759e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_in=spark.read.parquet('build/s_in/bizwiz_value_score_2.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "f4901f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_in.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "6f948c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in_schema=StructType([StructField('id', DoubleType(), True), StructField('pptitle', StringType(), True), StructField('ppdesc', StringType(), True), StructField('ppdetails', StringType(), True), StructField('ppfinancials', StringType(), True), StructField('pcategories', ArrayType(StringType(), True), True), StructField('COUNTY_NAME', StringType(), True), StructField('STATE_NAME', StringType(), True), StructField('price', DoubleType(), True), StructField('cash_flow', DoubleType(), True), StructField('gross_revenue', DoubleType(), True), StructField('established', DoubleType(), True), StructField('POVERTY_PERCENT', DoubleType(), True), StructField('MEDIAN_HOUSEHOLD_INCOME', DoubleType(), True), StructField('bizwiz_value_score', DoubleType(), True), StructField('bizwiz_class', LongType(), True), StructField('bizwiz_label', StringType(), True), StructField('label_num', LongType(), True), StructField('label', StringType(), True), StructField('Abs(established - sqrt(gross_revenue))', DoubleType(), True), StructField('Abs(sqrt(MEDIAN_HOUSEHOLD_INCOME) - sqrt(cash_flow))', DoubleType(), True), StructField('Abs(sqrt(cash_flow) - established)', DoubleType(), True), StructField('log(MEDIAN_HOUSEHOLD_INCOME + cash_flow)', DoubleType(), True), StructField('sqrt(established + sqrt(gross_revenue))', DoubleType(), True), StructField('Abs(POVERTY_PERCENT**2 - sqrt(cash_flow))', DoubleType(), True), StructField('1/(sqrt(MEDIAN_HOUSEHOLD_INCOME) + cash_flow)', DoubleType(), True), StructField('POVERTY_PERCENT**2*cash_flow', DoubleType(), True), StructField('1/(-sqrt(gross_revenue) + 1/POVERTY_PERCENT)', DoubleType(), True), StructField('sqrt(gross_revenue/POVERTY_PERCENT)', DoubleType(), True), StructField('POVERTY_PERCENT**3*sqrt(gross_revenue)', DoubleType(), True), StructField('sqrt(MEDIAN_HOUSEHOLD_INCOME)*established**3', DoubleType(), True), StructField('(sqrt(MEDIAN_HOUSEHOLD_INCOME) + established)**3', DoubleType(), True), StructField('__index_level_0__', LongType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "88da14f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_fit(self , data):\n",
    "    if(hasattr(vectorizer , 'vocabulary_')):\n",
    "        vocab = self.vocabulary_\n",
    "    else:\n",
    "        vocab = {}\n",
    "    self.fit(data)\n",
    "    vocab = list(set(vocab.keys()).union(set(self.vocabulary_ )))\n",
    "    self.vocabulary_ = {vocab[i] : i for i in range(len(vocab))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "305ba935",
   "metadata": {},
   "outputs": [],
   "source": [
    "CountVectorizer.partial_fit = partial_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "be3a801d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a943977",
   "metadata": {},
   "source": [
    "### Process \"Big\" datset in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "cc1bd95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def func(itr):\n",
    "#    for row in iter:\n",
    "#        print(row.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "2d54677e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_in.foreachPartition(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "5638dba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset=df_in.select(\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "0db228b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_proc(df, epoch_id):\n",
    "    X= df.select('ppdesc').collect() #.to_numpy()\n",
    "    print(X)\n",
    "    #y= df.price.to_numpy()\n",
    "    vectorizer.partial_fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "93d78b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://saturncloud.io/blog/how-to-use-foreach-and-foreachbatch-in-pyspark-to-write-to-database/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "43605dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#query = dataset.writeStream.outputMode(\"update\").foreachBatch(batch_proc).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "8fba57c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "2d0d9133",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_in=spark.readStream.format(\"parquet\").schema(df_in_schema).load('build/s_in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "8f81cc17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_in.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "4581bf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=sdf_in.select(\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "dfdd8dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.clientserver:There was an exception while executing the Python Proxy on the Python Side.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/sql/types.py\", line 2377, in __getattr__\n",
      "    idx = self.__fields__.index(item)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: 'lower' is not in list\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 617, in _call_proxy\n",
      "    return_value = getattr(self.pool[obj_id], method)(*params)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 120, in call\n",
      "    raise e\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 117, in call\n",
      "    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)\n",
      "  File \"/tmp/ipykernel_113/2433842204.py\", line 5, in batch_proc\n",
      "    vectorizer.partial_fit(X)\n",
      "  File \"/tmp/ipykernel_113/1478220754.py\", line 6, in partial_fit\n",
      "    self.fit(data)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/sklearn/feature_extraction/text.py\", line 1340, in fit\n",
      "    self.fit_transform(raw_documents)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/sklearn/base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/sklearn/feature_extraction/text.py\", line 1389, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/sklearn/feature_extraction/text.py\", line 1276, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "                   ^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/sklearn/feature_extraction/text.py\", line 110, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "          ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/sklearn/feature_extraction/text.py\", line 68, in _preprocess\n",
      "    doc = doc.lower()\n",
      "          ^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/sql/types.py\", line 2382, in __getattr__\n",
      "    raise AttributeError(item)\n",
      "AttributeError: lower\n",
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = dataset.writeStream.outputMode(\"update\").foreachBatch(batch_proc).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "3330c482",
   "metadata": {},
   "outputs": [],
   "source": [
    "#query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "a0195550",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47afa28f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
