{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd44f312",
   "metadata": {},
   "source": [
    "# 23.5 12 Design Your Deployment Solution Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d11217",
   "metadata": {},
   "source": [
    "![mlops](mlops.drawio.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d4412b",
   "metadata": {},
   "source": [
    "## overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85228353",
   "metadata": {},
   "source": [
    "This document describes a prototype of what the deployment would look like in production.\n",
    "\n",
    "* Web data is ingested into the data lake.\n",
    "* The data is validated and prepared for model training.\n",
    "* A model training job is started using SageMaker.\n",
    "* The trained model is evaluated and validated.\n",
    "* If the model performs well, it is registered in the model registry.\n",
    "* The model is deployed to production using SageMaker.\n",
    "* The production web application consumes the deployed model to generate predictions for users.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f658194",
   "metadata": {},
   "source": [
    "## orchestration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da4d168",
   "metadata": {},
   "source": [
    "The orchestration layer is responsible for automating the entire MLOps process, from data ingestion to model deployment. This can be done using a variety of tools, such as AWS CodePipeline, AWS Step Functions, or SageMaker Pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ddb1a1",
   "metadata": {},
   "source": [
    "### data ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffae31c8",
   "metadata": {},
   "source": [
    "The orchestration service periodcly starts jobs that run the web scraper and loads the data into the datalake for further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea646ae1",
   "metadata": {},
   "source": [
    "### data validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20ae361",
   "metadata": {},
   "source": [
    "The orchestration service triggers a job and raw data is run trhough a data validation step. This step ensures the expected data is presenent, and the data types and ranges are correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dda5c5c",
   "metadata": {},
   "source": [
    "### data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb138a88",
   "metadata": {},
   "source": [
    "The orchestration service triggers a job and validated data is run trhough a data preparation step. Data preparation is the process of cleaning, transforming, and enriching data to make it suitable for analysis. \n",
    "\n",
    "Data preparation typically involves the following steps:\n",
    "\n",
    "* Data cleaning: This step involves identifying and correcting errors and inconsistencies in the data. This may involve removing duplicate rows, correcting misspellings, and filling in missing values.\n",
    "\n",
    "* Feature engineering: This step involves creating new features from the existing data. This may involve transforming the data into different formats, aggregating the data, or creating new features that represent specific concepts or relationships.\n",
    "\n",
    "* Data scaling: This step involves scaling the data so that all of the variables are on the same scale. This is important for some machine learning algorithms, which are sensitive to the scale of the data.\n",
    "\n",
    "* Data splitting: This step involves splitting the data into training, validation, and test sets. The training set is used to train the machine learning model, the validation set is used to tune the model hyperparameters, and the test set is used to evaluate the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e27eaa7",
   "metadata": {},
   "source": [
    "### model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c320993",
   "metadata": {},
   "source": [
    "The orchestration service triggers a job and prepared data is run trhough a model training step. Once the model is trained on the latest dataset, it is serialized and saved into the model regestry along with any important metadata such as hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986fde21",
   "metadata": {},
   "source": [
    "### model validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ac7ef7",
   "metadata": {},
   "source": [
    "The orchestration service triggers a job and trained model is run trhough a model validation step. In this step, the model is evaluted and its perfomance metrics are captured in the model regestry. At this point if the model is good enough it may be automaticly promoted to production and deployed with a CICD pipeline. Alternatively, there may be a workflow that requires an expert to review the model in the regestry and decide to promote it to production.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bd66f3",
   "metadata": {},
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d0f356",
   "metadata": {},
   "source": [
    "The data layer is responsible for storing and managing all of the data that is used to train and deploy ML models The data is stored in a central location, typcally called a data lake, so that it can be easily accessed by all of the components of the MLOps pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335ecdc5",
   "metadata": {},
   "source": [
    "### web data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b02985e",
   "metadata": {},
   "source": [
    "Website data is the source for training data for the ml model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdd74a9",
   "metadata": {},
   "source": [
    "### data lake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c07f39",
   "metadata": {},
   "source": [
    "A typical AWS data lake implementation uses a combination of AWS services to store, analyze, and process large amounts of data. The most common services used are Amazon S3, AWS Lake Formation, Amazon Athena, and Amazon EMR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e00b2a",
   "metadata": {},
   "source": [
    "### model registry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a092d2e",
   "metadata": {},
   "source": [
    "The model registry is a central repository for storing and managing ML models. It should track the versioning of models, as well as their performance metrics. This allows data scientists and engineers to easily deploy the best performing model to production.\n",
    "\n",
    "A typical model registry on AWS is implemented using Amazon SageMaker Model Registry. SageMaker Model Registry is a centralized repository for storing and managing machine learning (ML) models. It allows data scientists and engineers to track the versioning of models, as well as their performance metrics and approval status. This makes it easy to deploy the best performing model to production and to track the performance of deployed models over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5dc41b",
   "metadata": {},
   "source": [
    "## production web application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47da495",
   "metadata": {},
   "source": [
    "The production web application is the end destination of the MLOps pipeline. It is the application that will consume the ML models and generate predictions for users.\n",
    "\n",
    "A typical production model service on AWS is implemented using Amazon SageMaker Real-Time Hosting. SageMaker Real-Time Hosting is a fully managed service that makes it easy to deploy and manage machine learning (ML) models in production. SageMaker Real-Time Hosting can be used to deploy models of any size and complexity, and it can be scaled to meet the demands of even the most demanding applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462b952e",
   "metadata": {},
   "source": [
    "## implementation estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e79d419",
   "metadata": {},
   "source": [
    "Here is a general overview of the resources, time, and money that can be expected to invest in implementing an MLOps architecture on AWS:\n",
    "\n",
    "Resources:\n",
    "\n",
    "AWS compute resources, such as Amazon EC2 instances and Amazon SageMaker endpoints\n",
    "AWS storage resources, such as Amazon S3 buckets and Amazon EBS volumes\n",
    "AWS networking resources, such as Amazon VPCs and Amazon Route 53 domains\n",
    "AWS data management resources, such as Amazon Glue and Amazon Athena\n",
    "AWS machine learning resources, such as Amazon SageMaker and Amazon Rekognition\n",
    "\n",
    "Time:\n",
    "\n",
    "The amount of time it takes to implement an MLOps architecture on AWS will vary depending on the size and complexity of the system. However, it is expected to spend at least a few weeks to a few months implementing a typical MLOps architecture.\n",
    "\n",
    "Money:\n",
    "\n",
    "It is expected to spend at least a few thousand dollars per month to implement and maintain a typical MLOps architecture.\n",
    "\n",
    "Considerations for reducing the cost of implementing an MLOps architecture on AWS:\n",
    "\n",
    "* Use spot instances to save up to 90% on the cost of EC2 instances.\n",
    "* Use reserved instances to save up to 75% on the cost of EC2 instances.\n",
    "* Use managed services, such as Amazon SageMaker and Amazon Glue, to reduce the operational overhead of managing your infrastructure.\n",
    "* Use serverless computing services, such as AWS Lambda, to reduce the cost of running code.\n",
    "* Use monitoring and logging services to identify and troubleshoot problems early.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
